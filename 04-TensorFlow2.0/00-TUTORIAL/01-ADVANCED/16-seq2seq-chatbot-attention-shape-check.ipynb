{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Attention\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypter Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size\n",
    "BATCH_SIZE = 64\n",
    "# Embedding Dimension\n",
    "EMBEDDING_DIM = 100\n",
    "# 1문장 당 30 단어\n",
    "TIME_STEPS = 30\n",
    "# Vocab 사전의 크기\n",
    "VOCAB_SIZE = 12638\n",
    "# hidden unit\n",
    "UNITS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 입력: (None, 30)\n",
      "------------------------------\n",
      "Embedding 출력: (None, 30, 100)\n",
      "------------------------------\n",
      "output: (None, 30, 128)\n",
      "hidden_state: (None, 128)\n",
      "cell_state: (None, 128)\n",
      "------------------------------\n",
      "encoder output: (None, 30, 128)\n"
     ]
    }
   ],
   "source": [
    "x = Input(shape=(TIME_STEPS))\n",
    "print(f'Data 입력: {x.get_shape()}')\n",
    "print('---'*10)\n",
    "\n",
    "x = Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=TIME_STEPS, name='Embedding')(x)\n",
    "print(f'Embedding 출력: {x.get_shape()}')\n",
    "print('---'*10)\n",
    "\n",
    "x, hidden_state, cell_state = LSTM(UNITS, return_state=True, return_sequences=True, name='LSTM')(x)\n",
    "\n",
    "print(f'output: {x.get_shape()}')\n",
    "print(f'hidden_state: {hidden_state.get_shape()}')\n",
    "print(f'cell_state: {cell_state.get_shape()}')\n",
    "print('---'*10)\n",
    "\n",
    "encoder_output = x\n",
    "print(f'encoder output: {encoder_output.get_shape()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**context_vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector = [h, c]\n",
    "# [(None, 128), (None, 128)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_inputs: (None, 30)\n",
      "------------------------------\n",
      "Embedding 출력: (None, 30, 100)\n",
      "------------------------------\n",
      "decoder_output: (None, 30, 128)\n",
      "hidden_state: (None, 128)\n",
      "cell_state: (None, 128)\n",
      "------------------------------\n",
      "decoder_output: (None, 30, 128)\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = context_vector\n",
    "\n",
    "decoder_inputs = Input(shape=(TIME_STEPS))\n",
    "print(f'decoder_inputs: {decoder_inputs.get_shape()}')\n",
    "print('---'*10)\n",
    "\n",
    "x = Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=TIME_STEPS)(decoder_inputs)\n",
    "print(f'Embedding 출력: {x.get_shape()}')\n",
    "print('---'*10)\n",
    "\n",
    "decoder_output, hidden_state, cell_state = LSTM(UNITS, return_state=True, return_sequences=True)(x)\n",
    "print(f'decoder_output: {decoder_output.get_shape()}')\n",
    "print(f'hidden_state: {hidden_state.get_shape()}')\n",
    "print(f'cell_state: {cell_state.get_shape()}')\n",
    "print('---'*10)\n",
    "\n",
    "print(f'decoder_output: {encoder_output.get_shape()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**key_value**\n",
    "\n",
    "encoder로 부터 나온 hidden_state와 decoder_output을 concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 128])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs[0].get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_value: (None, 30, 128)\n"
     ]
    }
   ],
   "source": [
    "key_value = tf.concat([encoder_inputs[0][:, tf.newaxis, :], decoder_output[:, :-1, :]], axis=1)   \n",
    "\n",
    "print(f'key_value: {key_value.get_shape()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_value: (None, 30, 128)\n",
      "encoder_output: (None, 30, 128)\n",
      "------------------------------\n",
      "attention_matrix: (None, 30, 128)\n"
     ]
    }
   ],
   "source": [
    "# 이전 hidden_state의 값을 concat으로 만든 vector와 encoder에서 나온 출력 값들로 attention을 구합니다.\n",
    "# key_value: (None, 30, 128)\n",
    "# encoder_output: (None, 30, 128)\n",
    "print(f'key_value: {key_value.get_shape()}')\n",
    "print(f'encoder_output: {encoder_output.get_shape()}')\n",
    "attention_matrix = Attention()([key_value, encoder_output])\n",
    "print('---'*10)\n",
    "print(f'attention_matrix: {attention_matrix.get_shape()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention: (None, 30, 128)\n",
      "decoder_output: (None, 30, 128)\n",
      "------------------------------\n",
      "final output (x): (None, 30, 256)\n"
     ]
    }
   ],
   "source": [
    "# 위에서 구한 attention_matrix와 decoder의 출력 값을 concat 합니다.\n",
    "x = tf.concat([decoder_output, attention_matrix], axis=-1)\n",
    "\n",
    "# (30, 128)\n",
    "print(f'attention: {attention_matrix.get_shape()}')\n",
    "# (30, 128)\n",
    "decoder_output.get_shape()\n",
    "print(f'decoder_output: {decoder_output.get_shape()}')\n",
    "print('---'*10)\n",
    "print(f'final output (x): {x.get_shape()}')\n",
    "# x.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 30, 12638])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Dense(VOCAB_SIZE, activation='softmax')(x)\n",
    "x.get_shape()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
